<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-guide/experiments/experiment-settings">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.0">
<title data-rh="true">Experiment settings | H2O LLM Studio | Docs</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://docs.h2o.ai/h2o-llmstudio/guide/experiments/experiment-settings"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="sitename" content="H2O LLM Studio | Docs"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Experiment settings | H2O LLM Studio | Docs"><meta data-rh="true" name="description" content="The settings for creating an experiment are grouped into the following sections:"><meta data-rh="true" property="og:description" content="The settings for creating an experiment are grouped into the following sections:"><link data-rh="true" rel="icon" href="/h2o-llmstudio/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docs.h2o.ai/h2o-llmstudio/guide/experiments/experiment-settings"><link data-rh="true" rel="alternate" href="https://docs.h2o.ai/h2o-llmstudio/guide/experiments/experiment-settings" hreflang="en"><link data-rh="true" rel="alternate" href="https://docs.h2o.ai/h2o-llmstudio/guide/experiments/experiment-settings" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://ADXS57JGWZ-dsn.algolia.net" crossorigin="anonymous"><link rel="search" type="application/opensearchdescription+xml" title="H2O LLM Studio | Docs" href="/h2o-llmstudio/opensearch.xml">

<link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons"><link rel="stylesheet" href="/h2o-llmstudio/assets/css/styles.2214709f.css">
<link rel="preload" href="/h2o-llmstudio/assets/js/runtime~main.b32a71ca.js" as="script">
<link rel="preload" href="/h2o-llmstudio/assets/js/main.19583cfc.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a href="https://docs.h2o.ai/haic-documentation/" target="_blank" rel="noopener noreferrer" class="navbar__brand"><div class="navbar__logo"><img src="/h2o-llmstudio/img/h2oai.png" alt="H2O AI Cloud logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/h2o-llmstudio/img/h2oai.png" alt="H2O AI Cloud logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Documentation</b></a><div class="navbarItems_fVuF"><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Platform</a><ul class="dropdown__menu"><li><a href="https://docs.h2o.ai/haic-documentation/" target="_blank" rel="noopener noreferrer" class="dropdown__link">HAIC Platform</a></li><li><a href="https://docs.h2o.ai/h2o-ai-cloud/index.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">AI App Store</a></li><li><a href="https://docs.h2o.ai/wave-mc-admin-center/" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O Admin Center</a></li><li><a href="https://docs.h2o.ai/h2o-drive/" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O Drive</a></li><li><a href="https://h2oai.github.io/featurestore/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Feature Store</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Applications</a><ul class="dropdown__menu"><li><a href="https://docs.h2o.ai/wave-apps/h2o-autodoc/" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O AutoDoc</a></li><li><a href="https://docs.h2o.ai/wave-apps/h2o-autoinsights/" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O AutoInsights</a></li><li><a href="https://h2oai.github.io/h2o-ai-cloud/docs/userguide/notebooks/" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O AI Notebooks</a></li><li><a href="https://docs.h2o.ai/wave-apps/ai-unit-consumption/" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O AI Unit Consumption</a></li><li><a href="https://docs.h2o.ai/h2o-drive/" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O Drive</a></li><li><a href="https://docs.h2o.ai/wave-apps/h2o-label-genie/" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O Label Genie</a></li><li><a href="https://docs.h2o.ai/wave-apps/h2o-model-analyzer/" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O Model Analyzer</a></li><li><a href="https://docs.h2o.ai/wave-apps/h2o-model-validation/" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O Model Validation</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Models</a><ul class="dropdown__menu"><li><a href="https://docs.h2o.ai/mlops/" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O MLOps</a></li><li><a href="https://docs.h2o.ai/wave-apps/h2o-model-analyzer/" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O Model Analyzer</a></li><li><a href="https://docs.h2o.ai/wave-apps/h2o-model-validation/" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O Model Validation</a></li><li><a href="https://docs.h2o.ai/h2o-escorer/index.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O eScorer</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">AI Engines</a><ul class="dropdown__menu"><li><a href="https://docs.h2o.ai/driverless-ai/1-10-lts/docs/userguide/index.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O Driverless AI</a></li><li><a href="https://docs.h2o.ai/h2o-document-ai/" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O Document AI</a></li><li><a href="https://docs.h2o.ai/h2o-hydrogen-torch/" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O Hydrogen Torch</a></li><li><a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/index.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">H2O-3</a></li></ul></div></div></div><div class="navbar__items navbar__items--right"><span class="versionLabel_pQWd"> </span><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button><div class="navbarButtons_f03l"><a href="https://h2o.ai/platform/ai-cloud/" target="_blank" rel="noopener noreferrer"><button class="button navbar-button navbar-button--primary">Learn More</button></a><a href="https://h2o.ai/demo/" target="_blank" rel="noopener noreferrer"><button class="button navbar-button navbar-button--secondary">Request a Demo</button></a></div><div class="toggle_vylO colorModeToggle_x44X"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/h2o-llmstudio/">H2O LLM Studio | Docs</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/h2o-llmstudio/get-started/what-is-h2o-llm-studio">Get started</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/h2o-llmstudio/concepts">Concepts</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/h2o-llmstudio/guide/datasets/data-connectors-format">Guide</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/h2o-llmstudio/guide/datasets/data-connectors-format">Datasets</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/h2o-llmstudio/guide/experiments/experiment-settings">Experiments</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/h2o-llmstudio/guide/experiments/experiment-settings">Experiment settings</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/h2o-llmstudio/guide/experiments/create-an-experiment">Create an experiment</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/h2o-llmstudio/guide/experiments/view-an-experiment">View and manage experiments</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/h2o-llmstudio/guide/experiments/compare-experiments">Compare experiments</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/h2o-llmstudio/guide/experiments/export-trained-model">Publish model to HuggingFace</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/h2o-llmstudio/guide/experiments/import-to-h2ogpt">Import a model to h2oGPT</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/h2o-llmstudio/faqs">FAQs</a></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/h2o-llmstudio/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Guide</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Experiments</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Experiment settings</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Experiment settings</h1><p>The settings for creating an experiment are grouped into the following sections: </p><ul><li><a href="#general-settings">General settings</a> </li><li><a href="#dataset-settings">Dataset settings</a></li><li><a href="#tokenizer-settings">Tokenizer settings</a></li><li><a href="#architecture-settings">Architecture settings</a></li><li><a href="#training-settings">Training settings</a></li><li><a href="#augmentation-settings">Augmentation settings</a></li><li><a href="#prediction-settings">Prediction settings</a></li><li><a href="#environment-settings">Environment settings</a></li><li><a href="#logging-settings">Logging settings</a></li></ul><p>The settings under each category are listed and described below.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="general-settings">General settings<a href="#general-settings" class="hash-link" aria-label="Direct link to General settings" title="Direct link to General settings">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dataset">Dataset<a href="#dataset" class="hash-link" aria-label="Direct link to Dataset" title="Direct link to Dataset">​</a></h3><p>It defines the dataset for the experiment.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="problem-type">Problem type<a href="#problem-type" class="hash-link" aria-label="Direct link to Problem type" title="Direct link to Problem type">​</a></h3><p>Defines the problem type of the experiment, which also defines the settings H2O LLM Studio displays for the experiment.</p><ul><li><p>Causal Language Modeling: Used to fine-tune large language models</p></li><li><p>Rlhf Language Modeling: Used to fine-tune RLHF language models</p></li><li><p>Sequence To Sequence Modeling: Used to fine-tune large sequence to sequence models</p></li><li><p>Causal Classification Modeling: Used to fine-tune causal classification models</p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="import-config-from-yaml">Import config from YAML<a href="#import-config-from-yaml" class="hash-link" aria-label="Direct link to Import config from YAML" title="Direct link to Import config from YAML">​</a></h3><p>Defines the <code>.yml</code> file that defines the experiment settings. </p><ul><li>H2O LLM Studio supports a <code>.yml</code> file import and export functionality. You can download the config settings of finished experiments, make changes, and re-upload them when starting a new experiment in any instance of H2O LLM Studio.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-name">Experiment name<a href="#experiment-name" class="hash-link" aria-label="Direct link to Experiment name" title="Direct link to Experiment name">​</a></h3><p>It defines the name of the experiment.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="llm-backbone">LLM backbone<a href="#llm-backbone" class="hash-link" aria-label="Direct link to LLM backbone" title="Direct link to LLM backbone">​</a></h3><p>The <strong>LLM Backbone</strong> option is the most important setting as it sets the pretrained model weights.</p><ul><li>Usually, it is good to use smaller architectures for quicker experiments and larger models when aiming for the highest accuracy</li><li>If possible, leverage backbones pre-trained closely to your use case</li><li>Any huggingface model can be used here (not limited to the ones in the dropdown list)</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="dataset-settings">Dataset settings<a href="#dataset-settings" class="hash-link" aria-label="Direct link to Dataset settings" title="Direct link to Dataset settings">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="train-dataframe">Train dataframe<a href="#train-dataframe" class="hash-link" aria-label="Direct link to Train dataframe" title="Direct link to Train dataframe">​</a></h3><p>Defines a <code>.csv</code> or <code>.pq</code> file containing a dataframe with training records that H2O LLM Studio uses to <em>train</em> the model.</p><ul><li>The records are combined into mini-batches when training the model.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="validation-strategy">Validation strategy<a href="#validation-strategy" class="hash-link" aria-label="Direct link to Validation strategy" title="Direct link to Validation strategy">​</a></h3><p>Specifies the validation strategy H2O LLM Studio uses for the experiment.</p><p>To properly assess the performance of your trained models, it is common practice to evaluate it on separate holdout data that the model has not seen during training. H2O LLM Studio allows you to specify different strategies for this task fitting your needs.</p><p>Options</p><ul><li><strong>Custom holdout validation</strong><ul><li>Specifies a separate holdout dataframe.</li></ul></li><li><strong>Automatic holdout validation</strong><ul><li>Allows to specify a holdout validation sample size that is automatically generated.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="validation-size">Validation size<a href="#validation-size" class="hash-link" aria-label="Direct link to Validation size" title="Direct link to Validation size">​</a></h3><p>Defines an optional relative size of the holdout validation set. H2O LLM Studio do automatically sample the selected
percentage from the full training data, and build a holdout dataset that the model is validated on.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-sample">Data sample<a href="#data-sample" class="hash-link" aria-label="Direct link to Data sample" title="Direct link to Data sample">​</a></h3><p>Defines the percentage of the data to use for the experiment. The default percentage is 100% (1).</p><p>Changing the default value can significantly increase the training speed. Still, it might lead to a substantially poor accuracy value. Using 100% (1) of the data for final models is highly recommended.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-column">Prompt column<a href="#prompt-column" class="hash-link" aria-label="Direct link to Prompt column" title="Direct link to Prompt column">​</a></h3><p>The column in the dataset containing the user prompt.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="answer-column">Answer column<a href="#answer-column" class="hash-link" aria-label="Direct link to Answer column" title="Direct link to Answer column">​</a></h3><p>The column in the dataset containing the expected output.</p><p>For classification, this needs to be an integer column containing the class label.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="parent-id-column">Parent ID column<a href="#parent-id-column" class="hash-link" aria-label="Direct link to Parent ID column" title="Direct link to Parent ID column">​</a></h3><p>An optional column specifying the parent id to be used for chained conversations. The value of this column needs to match an additional column with the name <code>id</code>. If provided, the prompt will be concatenated after preceeding parent rows.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="text-prompt-start">Text prompt start<a href="#text-prompt-start" class="hash-link" aria-label="Direct link to Text prompt start" title="Direct link to Text prompt start">​</a></h3><p>Optional text to prepend to each prompt.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="text-answer-separator">Text answer separator<a href="#text-answer-separator" class="hash-link" aria-label="Direct link to Text answer separator" title="Direct link to Text answer separator">​</a></h3><p>Optional text to append to each prompt / prepend to each answer.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="adaptive-kl-control">Adaptive Kl control<a href="#adaptive-kl-control" class="hash-link" aria-label="Direct link to Adaptive Kl control" title="Direct link to Adaptive Kl control">​</a></h2><p>Use adaptive KL control, otherwise linear.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="add-eos-token-to-prompt">Add EOS token to prompt<a href="#add-eos-token-to-prompt" class="hash-link" aria-label="Direct link to Add EOS token to prompt" title="Direct link to Add EOS token to prompt">​</a></h3><p>Adds EOS token at end of prompt.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="add-eos-token-to-answer">Add EOS token to answer<a href="#add-eos-token-to-answer" class="hash-link" aria-label="Direct link to Add EOS token to answer" title="Direct link to Add EOS token to answer">​</a></h3><p>Adds EOS token at end of answer.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="mask-prompt-labels">Mask prompt labels<a href="#mask-prompt-labels" class="hash-link" aria-label="Direct link to Mask prompt labels" title="Direct link to Mask prompt labels">​</a></h3><p>Whether to mask the prompt labels during training and only train on the loss of the answer.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="tokenizer-settings">Tokenizer settings<a href="#tokenizer-settings" class="hash-link" aria-label="Direct link to Tokenizer settings" title="Direct link to Tokenizer settings">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="max-length-prompt">Max length prompt<a href="#max-length-prompt" class="hash-link" aria-label="Direct link to Max length prompt" title="Direct link to Max length prompt">​</a></h3><p>The maximum sequence length of the prompt to use during training. In case of chained samples, this max length refers to a single prompt length in the chain.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="max-length-answer">Max length answer<a href="#max-length-answer" class="hash-link" aria-label="Direct link to Max length answer" title="Direct link to Max length answer">​</a></h3><p>The maximum sequence length of the answer to use during training. In case of chained samples, this max length refers to a single answer length in the chain.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="max-length">Max length<a href="#max-length" class="hash-link" aria-label="Direct link to Max length" title="Direct link to Max length">​</a></h3><p>Defines the maximum length of the input sequence H2O LLM Studio uses during model training. In other words, this setting specifies the maximum number of tokens an input text is transformed for model training.</p><p>A higher token count leads to higher memory usage that slows down training while increasing the probability of obtaining a higher accuracy value.</p><p>In case of Causal Language Modeling, this includes both prompt and answer, or all prompts and answers in case of chained samples. </p><p>In Sequence to Sequence Modeling, this refers to the length of the prompt, or the length of a full chained sample.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="add-prompt-answer-tokens">Add prompt answer tokens<a href="#add-prompt-answer-tokens" class="hash-link" aria-label="Direct link to Add prompt answer tokens" title="Direct link to Add prompt answer tokens">​</a></h3><p>Adds system, prompt and answer tokens as new tokens to the tokenizer. It is recommended to also set <code>Force Embedding Gradients</code> in this case.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="padding-quantile">Padding quantile<a href="#padding-quantile" class="hash-link" aria-label="Direct link to Padding quantile" title="Direct link to Padding quantile">​</a></h3><p>Defines the padding quantile H2O LLM Studio uses to select the maximum token length per batch. H2O LLM Studio performs padding of shorter sequences up to the specified padding quantile instead of the selected <strong>Max length</strong>. H2O LLM Studio truncates longer sequences.</p><ul><li>Lowering the quantile can significantly increase training runtime and reduce memory usage in unevenly distributed sequence lengths but can hurt performance </li><li>The setting depends on the batch size and should be adjusted accordingly </li><li>No padding is done in inference, and the selected <strong>Max Length</strong> is guaranteed</li><li>Setting to 0 disables padding</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="use-fast">Use fast<a href="#use-fast" class="hash-link" aria-label="Direct link to Use fast" title="Direct link to Use fast">​</a></h3><p>Whether or not to use a Fast tokenizer if possible. Some LLM backbones only offer certain types of tokenizers and changing this setting might be needed.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="architecture-settings">Architecture settings<a href="#architecture-settings" class="hash-link" aria-label="Direct link to Architecture settings" title="Direct link to Architecture settings">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="backbone-dtype">Backbone Dtype<a href="#backbone-dtype" class="hash-link" aria-label="Direct link to Backbone Dtype" title="Direct link to Backbone Dtype">​</a></h3><p>The datatype of the weights in the LLM backbone.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="gradient-checkpointing">Gradient Checkpointing<a href="#gradient-checkpointing" class="hash-link" aria-label="Direct link to Gradient Checkpointing" title="Direct link to Gradient Checkpointing">​</a></h3><p>Determines whether H2O LLM Studio activates gradient checkpointing (GC) when training the model. Starting GC reduces the video random access memory (VRAM) footprint at the cost of a longer runtime (an additional forward pass). Turning <strong>On</strong> GC enables it during the training process.</p><p><strong>Caution</strong>
Gradient checkpointing is an experimental setting that is not compatible with all backbones or all other settings.</p><p>Activating <em>GC</em> comes at the cost of a longer training time; for that reason, try training without <em>GC</em> first and only activate when experiencing <em>GPU out-of-memory (OOM)</em> errors.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="force-embedding-gradients">Force Embedding Gradients<a href="#force-embedding-gradients" class="hash-link" aria-label="Direct link to Force Embedding Gradients" title="Direct link to Force Embedding Gradients">​</a></h3><p>Whether to force the computation of gradients for the input embeddings during training. Useful for LORA.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="intermediate-dropout">Intermediate dropout<a href="#intermediate-dropout" class="hash-link" aria-label="Direct link to Intermediate dropout" title="Direct link to Intermediate dropout">​</a></h3><p>Defines the custom dropout rate H2O LLM Studio uses for intermediate layers in the transformer model.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="pretrained-weights">Pretrained weights<a href="#pretrained-weights" class="hash-link" aria-label="Direct link to Pretrained weights" title="Direct link to Pretrained weights">​</a></h3><p>Allows you to specify a local path to the pretrained weights.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="training-settings">Training settings<a href="#training-settings" class="hash-link" aria-label="Direct link to Training settings" title="Direct link to Training settings">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimizer">Optimizer<a href="#optimizer" class="hash-link" aria-label="Direct link to Optimizer" title="Direct link to Optimizer">​</a></h3><p>Defines the algorithm or method (optimizer) to use for model training. The selected algorithm or method defines how the model should change the attributes of the neural network, such as weights and learning rate. Optimizers solve optimization problems and make more accurate updates to attributes to reduce learning losses.</p><p>Options:</p><ul><li><strong>Adadelta</strong><ul><li>To learn about Adadelta, see <a href="https://arxiv.org/abs/1212.5701" target="_blank" rel="noopener noreferrer">ADADELTA: An Adaptive Learning Rate Method</a>. </li></ul></li><li><strong>Adam</strong><ul><li>To learn about Adam, see <a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener noreferrer">Adam: A Method for Stochastic Optimization</a>. </li></ul></li><li><strong>AdamW</strong><ul><li>To learn about AdamW, see <a href="https://arxiv.org/abs/1711.05101" target="_blank" rel="noopener noreferrer">Decoupled Weight Decay Regularization</a>.</li></ul></li><li><strong>AdamW8bit</strong><ul><li>To learn about AdamW, see <a href="https://arxiv.org/abs/1711.05101" target="_blank" rel="noopener noreferrer">Decoupled Weight Decay Regularization</a>.</li></ul></li><li><strong>RMSprop</strong> <ul><li>To learn about RMSprop, see <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener noreferrer">Neural Networks for Machine Learning</a>.</li></ul></li><li><strong>SGD</strong> <ul><li>H2O LLM Studio uses a stochastic gradient descent optimizer.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="learning-rate">Learning rate<a href="#learning-rate" class="hash-link" aria-label="Direct link to Learning rate" title="Direct link to Learning rate">​</a></h3><p>Defines the learning rate H2O LLM Studio uses when training the model, specifically when updating the neural network&#x27;s weights. The learning rate is the speed at which the model updates its weights after processing each mini-batch of data.</p><ul><li>Learning rate is an important setting to tune as it balances under- and overfitting.</li><li>The number of epochs highly impacts the optimal value of the learning rate.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="batch-size">Batch size<a href="#batch-size" class="hash-link" aria-label="Direct link to Batch size" title="Direct link to Batch size">​</a></h3><p>Defines the number of training examples a mini-batch uses during an iteration of the training model to estimate the error gradient before updating the model weights. <strong>Batch size</strong> defines the batch size used per a single GPU.</p><p>During model training, the training data is packed into mini-batches of a fixed size.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="epochs">Epochs<a href="#epochs" class="hash-link" aria-label="Direct link to Epochs" title="Direct link to Epochs">​</a></h3><p>Defines the number of epochs to train the model. In other words, it specifies the number of times the learning algorithm goes through the entire training dataset.</p><ul><li>The <strong>Epochs</strong> setting is an important setting to tune because it balances under- and overfitting.</li><li>The learning rate highly impacts the optimal value of the epochs.</li><li>H2O LLM Studio enables you to utilize a pre-trained model trained on zero epochs (where H2O LLM Studio does not train the model and the pretrained model (experiment) can be evaluated as-is):</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schedule">Schedule<a href="#schedule" class="hash-link" aria-label="Direct link to Schedule" title="Direct link to Schedule">​</a></h3><p>Defines the learning rate schedule H2O LLM Studio utilizes during model training. Specifying a learning rate schedule prevents the learning rate from staying the same. Instead, a learning rate schedule causes the learning rate to change over iterations, typically decreasing the learning rate to achieve a better model performance and training convergence.</p><p>Options</p><ul><li><strong>Constant</strong><ul><li>H2O LLM Studio applies a constant learning rate during the training process.</li></ul></li><li><strong>Cosine</strong><ul><li>H2O LLM Studio applies a cosine learning rate that follows the values of the cosine function.</li></ul></li><li><strong>Linear</strong><ul><li>H2O LLM Studio applies a linear learning rate that decreases the learning rate linearly.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="warmup-epochs">Warmup epochs<a href="#warmup-epochs" class="hash-link" aria-label="Direct link to Warmup epochs" title="Direct link to Warmup epochs">​</a></h3><p>Defines the number of epochs to warm up the learning rate where the learning rate should increase linearly from 0 to the desired learning rate. Can be a fraction of an epoch.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="weight-decay">Weight decay<a href="#weight-decay" class="hash-link" aria-label="Direct link to Weight decay" title="Direct link to Weight decay">​</a></h3><p>Defines the weight decay that H2O LLM Studio uses for the optimizer during model training.</p><p>Weight decay is a regularization technique that adds an L2 norm of all model weights to the loss function while increasing the probability of improving the model generalization.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="gradient-clip">Gradient clip<a href="#gradient-clip" class="hash-link" aria-label="Direct link to Gradient clip" title="Direct link to Gradient clip">​</a></h3><p>Defines the maximum norm of the gradients H2O LLM Studio specifies during model training. Defaults to <strong>0</strong>, no clipping. When a value greater than 0 is specified, H2O LLM Studio modifies the gradients during model training. H2O LLM Studio uses the specified value as an upper limit for the norm of the gradients, calculated using the Euclidean norm over all gradients per batch.</p><p>This setting can help model convergence when extreme gradient values cause high volatility of weight updates.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="grad-accumulation">Grad accumulation<a href="#grad-accumulation" class="hash-link" aria-label="Direct link to Grad accumulation" title="Direct link to Grad accumulation">​</a></h3><p>Defines the number of gradient accumulations before H2O LLM Studio updates the neural network weights during model training.</p><ul><li>Grad accumulation can be beneficial if only small batches are selected for training. With gradient accumulation, the loss and gradients are calculated after each batch, but it waits for the selected accumulations before updating the model weights. You can control the batch size through the <strong>Batch size</strong> setting.</li><li>Changing the default value of <em>Grad Accumulation</em> might require adjusting the learning rate and batch size.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="lora">Lora<a href="#lora" class="hash-link" aria-label="Direct link to Lora" title="Direct link to Lora">​</a></h3><p>Whether to use low rank approximations (LoRA) during training.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="lora-r">Lora R<a href="#lora-r" class="hash-link" aria-label="Direct link to Lora R" title="Direct link to Lora R">​</a></h3><p>The dimension of the matrix decomposition used in LoRA.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="lora-alpha">Lora Alpha<a href="#lora-alpha" class="hash-link" aria-label="Direct link to Lora Alpha" title="Direct link to Lora Alpha">​</a></h3><p>The scaling factor for the lora weights.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="lora-dropout">Lora dropout<a href="#lora-dropout" class="hash-link" aria-label="Direct link to Lora dropout" title="Direct link to Lora dropout">​</a></h3><p>The probability of applying dropout to the LoRA weights during training.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="lora-target-modules">Lora target modules<a href="#lora-target-modules" class="hash-link" aria-label="Direct link to Lora target modules" title="Direct link to Lora target modules">​</a></h3><p>The modules in the model to apply the LoRA approximation to. Defaults to all linear layers.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="save-best-checkpoint">Save best checkpoint<a href="#save-best-checkpoint" class="hash-link" aria-label="Direct link to Save best checkpoint" title="Direct link to Save best checkpoint">​</a></h3><p>Determines if H2O LLM Studio should save the model weights of the epoch exhibiting the best validation metric. When turned <strong>On</strong>, H2O LLM Studio saves the model weights for the epoch exhibiting the best validation metric. When turned <strong>Off</strong>, H2O LLM Studio saves the model weights after the last epoch is executed.</p><ul><li>This setting should be turned <strong>On</strong> with care as it has the potential to lead to overfitting of the validation data. </li><li>The default goal should be to attempt to tune models so that the last or very last epoch is the best epoch.  </li><li>Suppose an evident decline for later epochs is observed in logging. In that case, it is usually better to adjust hyperparameters, such as reducing the number of epochs or increasing regularization, instead of turning this setting <strong>On</strong>.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="evaluation-epochs">Evaluation epochs<a href="#evaluation-epochs" class="hash-link" aria-label="Direct link to Evaluation epochs" title="Direct link to Evaluation epochs">​</a></h3><p>Defines the number of epochs H2O LLM Studio uses before each validation loop for model training. In other words, it determines the frequency (in a number of epochs) to run the model evaluation on the validation data.</p><ul><li>Increasing the number of <em>Evaluation Epochs</em> can speed up an experiment.</li><li>The <strong>Evaluation epochs</strong> setting is available only if the following setting is turned <strong>Off</strong>: <strong>Save Best Checkpoint</strong>. </li><li>Can be a fraction of an epoch</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="evaluate-before-training">Evaluate before training<a href="#evaluate-before-training" class="hash-link" aria-label="Direct link to Evaluate before training" title="Direct link to Evaluate before training">​</a></h3><p>This option lets you evaluate the model before training, which can help you judge the quality of the LLM backbone before fine-tuning.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="train-validation-data">Train validation data<a href="#train-validation-data" class="hash-link" aria-label="Direct link to Train validation data" title="Direct link to Train validation data">​</a></h3><p>Defines whether the model should use the entire train and validation dataset during model training. When turned <strong>On</strong>, H2O LLM Studio uses the whole train dataset and validation data to train the model.</p><ul><li>H2O LLM Studio also evaluates the model on the provided validation fold. Validation is always only on the provided validation fold.</li><li>H2O LLM Studio uses both datasets for model training if you provide a train and validation dataset.<ul><li>To define a training dataset, use the <strong>Train dataframe</strong> setting.</li><li>To define a validation dataset, use the <strong>Validation dataframe</strong> setting.</li></ul></li><li>The <strong>Train validation data</strong> setting is only available if you turned the <strong>Save best checkpoint</strong> setting <strong>Off</strong>.</li><li>Turning <strong>On</strong> the <strong>Train validation data</strong> setting should produce a model that you can expect to perform better because H2O LLM Studio trained the model on more data. Though, also note that using the entire train dataset and validation dataset generally causes the model&#x27;s accuracy to be <em>overstated</em> as information from the validation data is incorporated into the model during the training process.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="use-rlhf">Use RLHF<a href="#use-rlhf" class="hash-link" aria-label="Direct link to Use RLHF" title="Direct link to Use RLHF">​</a></h3><p>Toggle to enable Reinforcement Learning with Human Feedback.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="reward-model">Reward model<a href="#reward-model" class="hash-link" aria-label="Direct link to Reward model" title="Direct link to Reward model">​</a></h3><p>The <strong>Reward Model</strong> option is gives control over the models weights that shall be used to score the active LLM during RLHF training.</p><ul><li>Any suited huggingface model can be used here (not limited to the ones in the dropdown list)</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="adaptive-kl-control-1">Adaptive KL control<a href="#adaptive-kl-control-1" class="hash-link" aria-label="Direct link to Adaptive KL control" title="Direct link to Adaptive KL control">​</a></h3><p>Use adaptive KL control, otherwise linear.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="initial-kl-coefficient">Initial KL coefficient<a href="#initial-kl-coefficient" class="hash-link" aria-label="Direct link to Initial KL coefficient" title="Direct link to Initial KL coefficient">​</a></h3><p>Initial KL penalty coefficient (used for adaptive and linear control).</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="kl-target">KL target<a href="#kl-target" class="hash-link" aria-label="Direct link to KL target" title="Direct link to KL target">​</a></h3><p>Target KL value for adaptive KL control.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="kl-horizon">KL Horizon<a href="#kl-horizon" class="hash-link" aria-label="Direct link to KL Horizon" title="Direct link to KL Horizon">​</a></h3><p>Horizon for adaptive KL control.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="advantages-gamma">Advantages gamma<a href="#advantages-gamma" class="hash-link" aria-label="Direct link to Advantages gamma" title="Direct link to Advantages gamma">​</a></h3><p>Gamma parameter for advantage calculation.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="advantages-lambda">Advantages Lambda<a href="#advantages-lambda" class="hash-link" aria-label="Direct link to Advantages Lambda" title="Direct link to Advantages Lambda">​</a></h3><p>Lambda parameter for advantage calculation.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="ppo-clip-policy">PPO clip policy<a href="#ppo-clip-policy" class="hash-link" aria-label="Direct link to PPO clip policy" title="Direct link to PPO clip policy">​</a></h3><p>Range for clipping in PPO policy gradient loss.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="ppo-clip-value">PPO clip value<a href="#ppo-clip-value" class="hash-link" aria-label="Direct link to PPO clip value" title="Direct link to PPO clip value">​</a></h3><p>Range for clipping values in loss calculation.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="scaling-factor-value-loss">Scaling factor value loss<a href="#scaling-factor-value-loss" class="hash-link" aria-label="Direct link to Scaling factor value loss" title="Direct link to Scaling factor value loss">​</a></h3><p>Scaling factor for value loss.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="ppo-epochs">PPO epochs<a href="#ppo-epochs" class="hash-link" aria-label="Direct link to PPO epochs" title="Direct link to PPO epochs">​</a></h3><p>Number of optimisation epochs per batch of samples.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="ppo-batch-size">PPO Batch Size<a href="#ppo-batch-size" class="hash-link" aria-label="Direct link to PPO Batch Size" title="Direct link to PPO Batch Size">​</a></h3><p>Number of samples optimized inside PPO together.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="ppo-generate-temperature">PPO generate temperature<a href="#ppo-generate-temperature" class="hash-link" aria-label="Direct link to PPO generate temperature" title="Direct link to PPO generate temperature">​</a></h3><p>This is the temperature that is used in the generate function during the PPO Rollout.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="offload-reward-model">Offload reward model<a href="#offload-reward-model" class="hash-link" aria-label="Direct link to Offload reward model" title="Direct link to Offload reward model">​</a></h3><p>When enabled, this will offload the reward model weights to CPU when not in use. This can be useful when training on a GPU with limited memory. The weights will be moved back to the GPU when needed.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="augmentation-settings">Augmentation settings<a href="#augmentation-settings" class="hash-link" aria-label="Direct link to Augmentation settings" title="Direct link to Augmentation settings">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="token-mask-probability">Token mask probability<a href="#token-mask-probability" class="hash-link" aria-label="Direct link to Token mask probability" title="Direct link to Token mask probability">​</a></h3><p>Defines the random probability of the input text tokens to be randomly masked during training. </p><ul><li>Increasing this setting can be helpful to avoid overfitting and apply regularization</li><li>Each token is randomly replaced by a masking token based on the specified probability</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="skip-parent-probability">Skip parent probability<a href="#skip-parent-probability" class="hash-link" aria-label="Direct link to Skip parent probability" title="Direct link to Skip parent probability">​</a></h3><p>If <code>Parent Column</code> is set, this random augmentation will skip parent concatenation during training at each parent with this specified probability.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="random-parent-probability">Random parent probability<a href="#random-parent-probability" class="hash-link" aria-label="Direct link to Random parent probability" title="Direct link to Random parent probability">​</a></h3><p>While training, each sample will be concatenated to a random other sample simulating unrelated chained conversations. Can be specified without using a <code>Parent Column</code>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="prediction-settings">Prediction settings<a href="#prediction-settings" class="hash-link" aria-label="Direct link to Prediction settings" title="Direct link to Prediction settings">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="metric">Metric<a href="#metric" class="hash-link" aria-label="Direct link to Metric" title="Direct link to Metric">​</a></h3><p>Defines the metric to evaluate the model&#x27;s performance. </p><p>We provide several metric options for evaluating the performance of your model.
In addition to the BLEU and the Perplexity score, we offer GPT metrics that utilize the OpenAI API to determine whether
the predicted answer is more favorable than the ground truth answer.
To use these metrics, you can either export your OpenAI API key as an environment variable before starting LLM Studio,
or you can specify it in the Settings Menu within the UI.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="min-length-inference">Min length inference<a href="#min-length-inference" class="hash-link" aria-label="Direct link to Min length inference" title="Direct link to Min length inference">​</a></h3><p>Defines the min length value H2O LLM Studio uses for the generated text.</p><ul><li>This setting impacts the evaluation metrics and should depend on the dataset and average output sequence length that is expected to be predicted.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="max-length-inference">Max length inference<a href="#max-length-inference" class="hash-link" aria-label="Direct link to Max length inference" title="Direct link to Max length inference">​</a></h3><p>Defines the max length value H2O LLM Studio uses for the generated text.</p><ul><li>Similar to the <strong>Max Length</strong> setting in the <em>tokenizer settings</em> section, this setting specifies the maximum number of tokens to predict for a given prediction sample.</li><li>This setting impacts the evaluation metrics and should depend on the dataset and average output sequence length that is expected to be predicted.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="batch-size-inference">Batch size inference<a href="#batch-size-inference" class="hash-link" aria-label="Direct link to Batch size inference" title="Direct link to Batch size inference">​</a></h3><p>Defines the size of a mini-batch uses during an iteration of the inference. <strong>Batch size</strong> defines the batch size used per GPU.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="do-sample">Do sample<a href="#do-sample" class="hash-link" aria-label="Direct link to Do sample" title="Direct link to Do sample">​</a></h3><p>Determines whether to sample from the next token distribution instead of choosing the token with the highest probability. If turned <strong>On</strong>, the next token in a predicted sequence is sampled based on the probabilities. If turned <strong>Off</strong>, the highest probability is always chosen.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="num-beams">Num beams<a href="#num-beams" class="hash-link" aria-label="Direct link to Num beams" title="Direct link to Num beams">​</a></h3><p>Defines the number of beams to use for beam search. <em>Num Beams</em> default value is 1  (a single beam); no beam search.</p><p>A higher <em>Num Beams</em> value can increase prediction runtime while potentially improving accuracy.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="temperature">Temperature<a href="#temperature" class="hash-link" aria-label="Direct link to Temperature" title="Direct link to Temperature">​</a></h3><p>Defines the temperature to use for sampling from the next token distribution during validation and inference. In other words, the defined temperature controls the randomness of predictions by scaling the logits before applying <a href="https://www.researchgate.net/figure/The-Gumbel-Softmax-distribution-interpolates-between-discrete-one-hot-encoded-categorical_fig4_309663606" target="_blank" rel="noopener noreferrer">softmax</a>. A higher temperature makes the distribution more random.</p><ul><li>Modify the temperature value if you have the <strong>Do Sample</strong> setting enabled (<strong>On</strong>).</li><li>To learn more about this setting, refer to the following article: <a href="https://huggingface.co/blog/how-to-generate" target="_blank" rel="noopener noreferrer">How to generate text: using different decoding methods for language generation with Transformers</a>.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="repetition-penalty">Repetition penalty<a href="#repetition-penalty" class="hash-link" aria-label="Direct link to Repetition penalty" title="Direct link to Repetition penalty">​</a></h3><p>The parameter for repetition penalty. 1.0 means no penalty. See <a href="https://arxiv.org/pdf/1909.05858.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1909.05858.pdf</a> for more details.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="stop-tokens">Stop tokens<a href="#stop-tokens" class="hash-link" aria-label="Direct link to Stop tokens" title="Direct link to Stop tokens">​</a></h3><p>Will stop generation at occurrence of these additional tokens; multiple tokens should be split by comma <code>,</code>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="top-k">Top K<a href="#top-k" class="hash-link" aria-label="Direct link to Top K" title="Direct link to Top K">​</a></h3><p>If &gt; 0, only keep the top k tokens with the highest probability (top-k filtering).</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="top-p">Top P<a href="#top-p" class="hash-link" aria-label="Direct link to Top P" title="Direct link to Top P">​</a></h3><p>If &lt; 1.0, only keep the top tokens with cumulative probability &gt;= top_p (nucleus filtering).</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="environment-settings">Environment settings<a href="#environment-settings" class="hash-link" aria-label="Direct link to Environment settings" title="Direct link to Environment settings">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="gpus">GPUs<a href="#gpus" class="hash-link" aria-label="Direct link to GPUs" title="Direct link to GPUs">​</a></h3><p>Determines the list of GPUs H2O LLM Studio can use for the experiment. GPUs are listed by name, referring to their system ID (starting from 1).</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="mixed-precision">Mixed precision<a href="#mixed-precision" class="hash-link" aria-label="Direct link to Mixed precision" title="Direct link to Mixed precision">​</a></h3><p>Determines whether to use mixed-precision. When turned <strong>Off</strong>, H2O LLM Studio does not use mixed-precision.</p><p>Mixed-precision is a technique that helps decrease memory consumption and increases training speed.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="compile-model">Compile model<a href="#compile-model" class="hash-link" aria-label="Direct link to Compile model" title="Direct link to Compile model">​</a></h3><p>Compiles the model with Torch. Experimental!</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="find-unused-parameters">Find unused parameters<a href="#find-unused-parameters" class="hash-link" aria-label="Direct link to Find unused parameters" title="Direct link to Find unused parameters">​</a></h3><p>In Distributed Data Parallel (DDP) mode, <code>prepare_for_backward()</code> is called at the end of DDP forward pass. It traverses the autograd graph to find unused parameters when <code>find_unused_parameters</code> is set to True in DDP constructor.</p><p>Note that traversing the autograd graph introduces extra overheads, so applications should only set to True when necessary.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="trust-remote-code">Trust remote code<a href="#trust-remote-code" class="hash-link" aria-label="Direct link to Trust remote code" title="Direct link to Trust remote code">​</a></h3><p>Trust remote code. This can be necessary for some models that use code which is not (yet) part of the <code>transformers</code> package. Should always be checked with this option being switched <strong>Off</strong> first.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="number-of-workers">Number of workers<a href="#number-of-workers" class="hash-link" aria-label="Direct link to Number of workers" title="Direct link to Number of workers">​</a></h3><p>Defines the number of workers H2O LLM Studio uses for the <em>DataLoader</em>. In other words, it defines the number of CPU processes to use when reading and loading data to GPUs during model training.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seed">Seed<a href="#seed" class="hash-link" aria-label="Direct link to Seed" title="Direct link to Seed">​</a></h3><p>Defines the random seed value that H2O LLM Studio uses during model training. It defaults to -1, an arbitrary value. When the value is modified (not -1), the random seed allows results to be reproducible—defining a seed aids in obtaining predictable and repeatable results every time. Otherwise, not modifying the default seed value (-1) leads to random numbers at every invocation.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="logging-settings">Logging settings<a href="#logging-settings" class="hash-link" aria-label="Direct link to Logging settings" title="Direct link to Logging settings">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="logger">Logger<a href="#logger" class="hash-link" aria-label="Direct link to Logger" title="Direct link to Logger">​</a></h3><p>Defines the logger type that H2O LLM Studio uses for model training</p><p>Options</p><ul><li><strong>None</strong><ul><li>H2O LLM Studio does not use any logger.</li></ul></li><li><strong>Neptune</strong><ul><li>H2O LLM Studio uses Neptune as a logger to track the experiment. To use Neptune, you must specify a <strong>Neptune API token</strong> and a <strong>Neptune project</strong>.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="number-of-texts">Number of texts<a href="#number-of-texts" class="hash-link" aria-label="Direct link to Number of texts" title="Direct link to Number of texts">​</a></h3><div></div><hr><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>Feedback</div><div class="admonitionContent_S0QG"><ul><li><a href="https://github.com/h2oai/docs-issues-requests/issues/new?assignees=sherenem&amp;labels=llmstudio&amp;body=%23%23%23%20Documentation%20issue%2Frequest%0A%0A%3C!--%20Please%20provide%20a%20clear%20and%20concise%20description%20of%20the%20documentation%20issue%2Frequest%20--%3E%0A%0A%23%23%23%20Additional%20context%0A%0A%3C!--%20Please%20add%20any%20other%20context%20about%20the%20issue%2Frequest%20here%20(e.g.%2C%20images)%20--%3E%0A%0A%23%23%23%20Page%20details%20%0A%0A-%20Application%20name%3A%20H2O LLM Studio | Docs%0A-%20Application%20version%3A%200.0.0%0A-%20Page%20title%3A%20/h2o-llmstudio/guide/experiments/experiment-settings%20&amp;title=%5BHAIC-APP%5D" target="_blank" rel="noopener noreferrer">Submit and view feedback for this page</a></li><li>Send feedback about H2O LLM Studio | Docs to <a href="mailto:cloud-feedback@h2o.ai" target="_blank" rel="noopener noreferrer">cloud-feedback@h2o.ai</a></li></ul></div></div></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/h2o-llmstudio/guide/datasets/merge-datasets"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Merge datasets</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/h2o-llmstudio/guide/experiments/create-an-experiment"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Create an experiment</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#general-settings" class="table-of-contents__link toc-highlight">General settings</a><ul><li><a href="#dataset" class="table-of-contents__link toc-highlight">Dataset</a></li><li><a href="#problem-type" class="table-of-contents__link toc-highlight">Problem type</a></li><li><a href="#import-config-from-yaml" class="table-of-contents__link toc-highlight">Import config from YAML</a></li><li><a href="#experiment-name" class="table-of-contents__link toc-highlight">Experiment name</a></li><li><a href="#llm-backbone" class="table-of-contents__link toc-highlight">LLM backbone</a></li></ul></li><li><a href="#dataset-settings" class="table-of-contents__link toc-highlight">Dataset settings</a><ul><li><a href="#train-dataframe" class="table-of-contents__link toc-highlight">Train dataframe</a></li><li><a href="#validation-strategy" class="table-of-contents__link toc-highlight">Validation strategy</a></li><li><a href="#validation-size" class="table-of-contents__link toc-highlight">Validation size</a></li><li><a href="#data-sample" class="table-of-contents__link toc-highlight">Data sample</a></li><li><a href="#prompt-column" class="table-of-contents__link toc-highlight">Prompt column</a></li><li><a href="#answer-column" class="table-of-contents__link toc-highlight">Answer column</a></li><li><a href="#parent-id-column" class="table-of-contents__link toc-highlight">Parent ID column</a></li><li><a href="#text-prompt-start" class="table-of-contents__link toc-highlight">Text prompt start</a></li><li><a href="#text-answer-separator" class="table-of-contents__link toc-highlight">Text answer separator</a></li></ul></li><li><a href="#adaptive-kl-control" class="table-of-contents__link toc-highlight">Adaptive Kl control</a><ul><li><a href="#add-eos-token-to-prompt" class="table-of-contents__link toc-highlight">Add EOS token to prompt</a></li><li><a href="#add-eos-token-to-answer" class="table-of-contents__link toc-highlight">Add EOS token to answer</a></li><li><a href="#mask-prompt-labels" class="table-of-contents__link toc-highlight">Mask prompt labels</a></li></ul></li><li><a href="#tokenizer-settings" class="table-of-contents__link toc-highlight">Tokenizer settings</a><ul><li><a href="#max-length-prompt" class="table-of-contents__link toc-highlight">Max length prompt</a></li><li><a href="#max-length-answer" class="table-of-contents__link toc-highlight">Max length answer</a></li><li><a href="#max-length" class="table-of-contents__link toc-highlight">Max length</a></li><li><a href="#add-prompt-answer-tokens" class="table-of-contents__link toc-highlight">Add prompt answer tokens</a></li><li><a href="#padding-quantile" class="table-of-contents__link toc-highlight">Padding quantile</a></li><li><a href="#use-fast" class="table-of-contents__link toc-highlight">Use fast</a></li></ul></li><li><a href="#architecture-settings" class="table-of-contents__link toc-highlight">Architecture settings</a><ul><li><a href="#backbone-dtype" class="table-of-contents__link toc-highlight">Backbone Dtype</a></li><li><a href="#gradient-checkpointing" class="table-of-contents__link toc-highlight">Gradient Checkpointing</a></li><li><a href="#force-embedding-gradients" class="table-of-contents__link toc-highlight">Force Embedding Gradients</a></li><li><a href="#intermediate-dropout" class="table-of-contents__link toc-highlight">Intermediate dropout</a></li><li><a href="#pretrained-weights" class="table-of-contents__link toc-highlight">Pretrained weights</a></li></ul></li><li><a href="#training-settings" class="table-of-contents__link toc-highlight">Training settings</a><ul><li><a href="#optimizer" class="table-of-contents__link toc-highlight">Optimizer</a></li><li><a href="#learning-rate" class="table-of-contents__link toc-highlight">Learning rate</a></li><li><a href="#batch-size" class="table-of-contents__link toc-highlight">Batch size</a></li><li><a href="#epochs" class="table-of-contents__link toc-highlight">Epochs</a></li><li><a href="#schedule" class="table-of-contents__link toc-highlight">Schedule</a></li><li><a href="#warmup-epochs" class="table-of-contents__link toc-highlight">Warmup epochs</a></li><li><a href="#weight-decay" class="table-of-contents__link toc-highlight">Weight decay</a></li><li><a href="#gradient-clip" class="table-of-contents__link toc-highlight">Gradient clip</a></li><li><a href="#grad-accumulation" class="table-of-contents__link toc-highlight">Grad accumulation</a></li><li><a href="#lora" class="table-of-contents__link toc-highlight">Lora</a></li><li><a href="#lora-r" class="table-of-contents__link toc-highlight">Lora R</a></li><li><a href="#lora-alpha" class="table-of-contents__link toc-highlight">Lora Alpha</a></li><li><a href="#lora-dropout" class="table-of-contents__link toc-highlight">Lora dropout</a></li><li><a href="#lora-target-modules" class="table-of-contents__link toc-highlight">Lora target modules</a></li><li><a href="#save-best-checkpoint" class="table-of-contents__link toc-highlight">Save best checkpoint</a></li><li><a href="#evaluation-epochs" class="table-of-contents__link toc-highlight">Evaluation epochs</a></li><li><a href="#evaluate-before-training" class="table-of-contents__link toc-highlight">Evaluate before training</a></li><li><a href="#train-validation-data" class="table-of-contents__link toc-highlight">Train validation data</a></li><li><a href="#use-rlhf" class="table-of-contents__link toc-highlight">Use RLHF</a></li><li><a href="#reward-model" class="table-of-contents__link toc-highlight">Reward model</a></li><li><a href="#adaptive-kl-control-1" class="table-of-contents__link toc-highlight">Adaptive KL control</a></li><li><a href="#initial-kl-coefficient" class="table-of-contents__link toc-highlight">Initial KL coefficient</a></li><li><a href="#kl-target" class="table-of-contents__link toc-highlight">KL target</a></li><li><a href="#kl-horizon" class="table-of-contents__link toc-highlight">KL Horizon</a></li><li><a href="#advantages-gamma" class="table-of-contents__link toc-highlight">Advantages gamma</a></li><li><a href="#advantages-lambda" class="table-of-contents__link toc-highlight">Advantages Lambda</a></li><li><a href="#ppo-clip-policy" class="table-of-contents__link toc-highlight">PPO clip policy</a></li><li><a href="#ppo-clip-value" class="table-of-contents__link toc-highlight">PPO clip value</a></li><li><a href="#scaling-factor-value-loss" class="table-of-contents__link toc-highlight">Scaling factor value loss</a></li><li><a href="#ppo-epochs" class="table-of-contents__link toc-highlight">PPO epochs</a></li><li><a href="#ppo-batch-size" class="table-of-contents__link toc-highlight">PPO Batch Size</a></li><li><a href="#ppo-generate-temperature" class="table-of-contents__link toc-highlight">PPO generate temperature</a></li><li><a href="#offload-reward-model" class="table-of-contents__link toc-highlight">Offload reward model</a></li></ul></li><li><a href="#augmentation-settings" class="table-of-contents__link toc-highlight">Augmentation settings</a><ul><li><a href="#token-mask-probability" class="table-of-contents__link toc-highlight">Token mask probability</a></li><li><a href="#skip-parent-probability" class="table-of-contents__link toc-highlight">Skip parent probability</a></li><li><a href="#random-parent-probability" class="table-of-contents__link toc-highlight">Random parent probability</a></li></ul></li><li><a href="#prediction-settings" class="table-of-contents__link toc-highlight">Prediction settings</a><ul><li><a href="#metric" class="table-of-contents__link toc-highlight">Metric</a></li><li><a href="#min-length-inference" class="table-of-contents__link toc-highlight">Min length inference</a></li><li><a href="#max-length-inference" class="table-of-contents__link toc-highlight">Max length inference</a></li><li><a href="#batch-size-inference" class="table-of-contents__link toc-highlight">Batch size inference</a></li><li><a href="#do-sample" class="table-of-contents__link toc-highlight">Do sample</a></li><li><a href="#num-beams" class="table-of-contents__link toc-highlight">Num beams</a></li><li><a href="#temperature" class="table-of-contents__link toc-highlight">Temperature</a></li><li><a href="#repetition-penalty" class="table-of-contents__link toc-highlight">Repetition penalty</a></li><li><a href="#stop-tokens" class="table-of-contents__link toc-highlight">Stop tokens</a></li><li><a href="#top-k" class="table-of-contents__link toc-highlight">Top K</a></li><li><a href="#top-p" class="table-of-contents__link toc-highlight">Top P</a></li></ul></li><li><a href="#environment-settings" class="table-of-contents__link toc-highlight">Environment settings</a><ul><li><a href="#gpus" class="table-of-contents__link toc-highlight">GPUs</a></li><li><a href="#mixed-precision" class="table-of-contents__link toc-highlight">Mixed precision</a></li><li><a href="#compile-model" class="table-of-contents__link toc-highlight">Compile model</a></li><li><a href="#find-unused-parameters" class="table-of-contents__link toc-highlight">Find unused parameters</a></li><li><a href="#trust-remote-code" class="table-of-contents__link toc-highlight">Trust remote code</a></li><li><a href="#number-of-workers" class="table-of-contents__link toc-highlight">Number of workers</a></li><li><a href="#seed" class="table-of-contents__link toc-highlight">Seed</a></li></ul></li><li><a href="#logging-settings" class="table-of-contents__link toc-highlight">Logging settings</a><ul><li><a href="#logger" class="table-of-contents__link toc-highlight">Logger</a></li><li><a href="#number-of-texts" class="table-of-contents__link toc-highlight">Number of texts</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">About us</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://h2o.ai/" target="_blank" rel="noopener noreferrer" class="footer__link-item">H2O.AI<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://h2o.ai/company/contact-us/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Contact us<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Legal</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://h2o.ai/legal/privacy/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy policy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://h2o.ai/insights/responsible-ai/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Compliance &amp; responsible AI<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://twitter.com/h2oai" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/company/h2oai" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.youtube.com/c/H2Oai" target="_blank" rel="noopener noreferrer" class="footer__link-item">YouTube<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 <a href="https://h2o.ai" style="color:#FEC925">H2O.ai</a>, Inc.</div></div></div></footer></div>
<script src="/h2o-llmstudio/assets/js/runtime~main.b32a71ca.js"></script>
<script src="/h2o-llmstudio/assets/js/main.19583cfc.js"></script>
</body>
</html>